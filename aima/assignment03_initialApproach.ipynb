{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\nsys.path.append(\"./AIMA\")\nfrom search import *\nfrom agents import*",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Tpiece(Thing): #Create a movable piece class\n    pass\n\nclass Mobs(Thing):\n    pass\n\nclass Oobs(Thing):\n    pass\n\nclass Board(Environment): #Create an environment like last assignment\n    pass",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class PSA(SimpleProblemSolvingAgentProgram):\n    def __call__(self, board): #AIMA overrided function because it had some things wrong\n        self.percept = board.things #percept will just be the list of objects in the board environment\n        self.state = self.update_state(self.percept)\n        if not self.seq:\n            goal = self.formulate_goal(self.state)\n            problem = self.formulate_problem(self.state, goal)\n            self.seq.append(self.search(problem))\n            #This was originally self.seq = blabla in AIMA which obviously gives an error when trying to pop it later wtf AIMA\n            if not self.seq:\n                return None\n        return self.seq.pop(0) #self.seq goes back empty for working the next goal\n    \n    def update_state(self, percept): #from board.things it builds a tuple of tuples to represent the initial state of the current step\n        #Format: (('Identification char', (location)), next object of the percept in the same format)\n        #Example: ( ('A', (1, 1)), ('T', (5, 2)) ) <- means a state with agent in 1, 1 and movable piece in 5, 2\n        #Output needs to be tuple because the AIMA graph search function needs hashable data structures to add to the explored set\n        \n        state = {'M': [], 'O': []} #Easier to build the state as a dict\n        for p in percept:\n            if isinstance(p, Agent):\n                state['A'] = p.location\n            elif isinstance(p, Tpiece):\n                state['T'] = p.location\n            elif isinstance(p, Mobs):\n                state['M'].append(p.location)\n            elif isinstance(p, Oobs):\n                state['O'].append(p.location)\n            \n        temp = {}\n        temp['A'] = state['A']\n        temp['T'] = state['T']\n        temp['M'] = tuple(state['M'])\n        temp['O'] = tuple(state['O'])\n        \n        state = []\n        for k in temp.items():\n            state.append(k)\n                \n        state = tuple(state) #convert the state to tuple\n        return state\n\n    def formulate_goal(self, state): #returns the desired state of the current step\n        temp = dict(state)\n        #converting the state temporarily to a dictionary makes it easier to manip the data, example: {'A': (1,1), 'T': (5, 5)}\n        \n        if temp['A'][0] == temp['T'][0] and temp['A'][1] == temp['T'][1] - 1: #If the agent is already to the left of the movable piece\n            temp['A'] = (5, 4)\n            temp['T'] = (5, 5)\n            #The desired state would be to finish the job and have the piece be in 5, 5\n        else: #The agent is not to the left of the piece yet\n            temp['A'] = (temp['T'][0], temp['T'][1] - 1)\n            #The desired state is to initially position the agent to the left of the piece\n\n        #This next block simply builds a proper tuple formatted state from the edited dictionary\n        goal = []\n        for k in temp.items():\n            goal.append(k)\n        goal = tuple(goal)\n        return goal\n        '''TODO: obviously develop this function so it can formulate proper goals in all boards. Board 1 only requires \"2 goals\"\n        which is first to approach the piece and then push it, but some boards will require previous goals such as clearing\n        a path from obstacles'''\n\n    def formulate_problem(self, state, goal): #create a problem object with the initial state and goal state for this step\n        problem = PushnPull(state, goal)\n        return problem\n    \n    def search(self, problem): #Uses some search algorithm included in AIMA to solve the problem\n        return graph_search(problem, [])\n        ",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class PushnPull(Problem):\n    def actions(self, state):\n        \"\"\"Return the actions that can be executed in the given\n        state. The result would typically be a list, but if there are\n        many actions, consider yielding them one at a time in an\n        iterator, rather than building them all at once.\"\"\"\n        actions = []\n        temp = dict(state)\n        #converting the state temporarily to a dictionary makes it easier to manip the data, example: {'A': (1,1), 'T': (5, 5)}\n        \n        if temp['A'][0] == temp['T'][0] and temp['A'][1] == temp['T'][1] - 1: #if agent is to the left of the piece\n            actions.append('pushRight')\n        \n        else: #Agent still far from the piece\n            if temp['A'][1] == 1: #moving left is out of the grid so its illegal \n                actions.append('moveRight')\n            elif temp['A'][1] == 6: #moving right is out of the grid so its illegal\n                actions.append('moveLeft')\n            else: #any horizontal move action is legal\n                actions.append('moveRight')\n                actions.append('moveLeft')\n            \n            if temp['A'][0] == 1: #moving up is out of the grid so its illegal\n                actions.append('moveDown')\n            elif temp['A'][0] == 6: #moving down is out of the grid so its illegal\n                actions.append('moveUp')\n            else: #any vetical move action is legal\n                actions.append('moveDown')\n                actions.append('moveUp')\n            \n        return actions\n        '''TODO: obviously implement more actions for the other scenarios and goals that arise in different boards'''\n    \n    def result(self, state, action):\n        \"\"\"Return the state that results from executing the given\n        action in the given state. The action must be one of\n        self.actions(state).\"\"\"\n        temp = dict(state)\n        #converting the state temporarily to a dictionary makes it easier to manip the data, example: {'A': (1,1), 'T': (5, 5)}\n        \n        if action == 'pushRight':\n            temp['A'] = (temp['A'][0], temp['A'][1] + 1)\n            temp['T'] = (temp['T'][0], temp['T'][1] + 1)\n            #The new state has both the piece and the agent one cell to the right\n            \n        else:\n            if action == 'moveRight':\n                temp['A'] = (temp['A'][0], temp['A'][1] + 1)\n            elif action == 'moveLeft':\n                temp['A'] = (temp['A'][0], temp['A'][1] - 1)\n            elif action == 'moveUp':\n                temp['A'] = (temp['A'][0] - 1, temp['A'][1])\n            elif action == 'moveDown':\n                temp['A'] = (temp['A'][0] + 1, temp['A'][1])\n            #The new state has the agent move accordingly and the piece position is unchanged\n        \n        #This next block simply builds a proper tuple formatted state from the edited dictionary\n        result = []\n        for k in temp.items():\n            result.append(k)\n        result = tuple(result)\n        return result\n        '''TODO: obviously handle the proper state changes that arise from other actions and scenarios not implemented yet in\n        self.actions()'''\n    \n    def goal_test(self, state):\n        \"\"\"Return True if the state is a goal. The default method compares the\n        state to self.goal or checks for state in self.goal if it is a\n        list, as specified in the constructor. Override this method if\n        checking against a single self.goal is not enough.\"\"\"\n        if state == self.goal:\n            return True\n        else:\n            return False",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def graph_search(problem, frontier): #This is just a default AIMA search algorithm which is ran from an agent's search method\n    \"\"\"Search through the successors of a problem to find a goal.\n    The argument frontier should be an empty queue.\n    If two paths reach a state, only use the first one. [Figure 3.7]\"\"\"\n    frontier.append(Node(problem.initial))\n    explored = set()\n    while frontier:\n        node = frontier.pop()\n        if problem.goal_test(node.state):\n            return node\n        explored.add(node.state)\n        frontier.extend(child for child in node.expand(problem)\n                        if child.state not in explored and\n                        child not in frontier)\n    return None",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#This is the process for using the code above and solving board 1 of the assignment\n\n#Create an empty board, a movable piece and an agent\nboard1 = Board()\nt1 = Tpiece()\nmike = PSA()\n\n#Add things to the board/env as specified for board1 in the assignment\nboard1.add_thing(mike, (1, 1))\nboard1.add_thing(t1, (5, 2))\n\nprint(board1.things)\nmike.update_state(board1.things)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[<Agent>, <Tpiece>]\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "(('A', (1, 1)), ('T', (5, 2)), ('M', ()), ('O', ()))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#This is the process for using the code above and solving board 1 of the assignment\n\n#Create an empty board, a movable piece and an agent\nboard2 = Board()\nt1 = Tpiece()\nm1 = Mobs()\nm2 = Mobs()\nm3 = Mobs()\no1 = Oobs()\no2 = Oobs()\nmike = PSA()\n\n#Add things to the board/env as specified for board1 in the assignment\nboard2.add_thing(mike, (5, 1))\nboard2.add_thing(t1, (2, 3))\nboard2.add_thing(m1, (3, 2))\nboard2.add_thing(m2, (3, 3))\nboard2.add_thing(m3, (3, 4))\nboard2.add_thing(o1, (3, 1))\nboard2.add_thing(o2, (3, 5))\n\nprint(board2.things)\nmike.update_state(board2.things)\n\n\n",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[<Agent>, <Tpiece>, <Mobs>, <Mobs>, <Mobs>, <Oobs>, <Oobs>]\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "(('A', (5, 1)),\n ('T', (2, 3)),\n ('M', ((3, 2), (3, 3), (3, 4))),\n ('O', ((3, 1), (3, 5))))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Reach the first goal state (agent wants to go near the piece to move it) and save the resulting node class object to a variable\nfirst_goal = mike(board1) #This executes the __call__ method of the agent which is where the whole process starts\n\nprint(\"Sequence of states to get to the left side of the movable piece:\")\nprint(str(first_goal.path()) + \"\\n\") #each node has stored the sequence of nodes used to get to that node\nprint(\"Sequence of actions to get to the left side of the movable piece:\")\nprint(str(first_goal.solution()) + \"\\n\") #each node has stored the sequence of actions to get to that node\n\n#Now we need to achieve the second goal which is to push the piece to the target\n#First lets update the environment/board (this could be done from a fucntion inside the agent or somewhere else, putting it here provisionally)\nboard1.things = [] #clean the old version of the env\ntemp = dict(first_goal.path()[len(first_goal.path()) - 1].state) #get the last state reached (the previous goal state)\nfor k in temp.items(): #Use the previous goal state to update the env/board\n    if k[0] == 'A':\n        board1.add_thing(mike, k[1]) #agent is now in 5, 1 to the left of the piece\n    elif k[0] == 'T':\n        board1.add_thing(t1, k[1]) #piece actually hasn't moved yet from 5, 2 but its checked again regardless\n#TODO: graphical representation of the old board and the new one after each step (after each goal state is reached)\n\n#Reach the second goal state (piece is in target and agent is 1 cell to the left of target) and save the node class object to a variable\nsecond_goal = mike(board1)\n\nprint(\"Sequence of states to push the movable piece to the target\")\nprint(str(second_goal.path()) + \"\\n\")\nprint(\"Sequence of actions to push the movable piece to the target\")\nprint(str(second_goal.solution()) + \"\\n\")\n\nprint(\"Board 1 solved!\")\n#Obviously with more complex boards, more steps will be needed (more \"subgoals\"), here there were only 2 subgoals",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'tuple'>\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fdd0268c6938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test = {'A': (1, 1), 'M': [(2, 2), (3, 3)]}\ntemp = {}\ntemp['A'] = test['A']\ntemp['M'] = tuple(test['M'])\ntemp",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "{'A': (1, 1), 'M': ((2, 2), (3, 3))}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language": "fsharp",
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}