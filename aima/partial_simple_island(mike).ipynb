{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next cell runs a simple reflex agent program in a partially observable environment. Output depicts all the steps in a 50 iteration run with 3 items of each kind placed randomly in the environment "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Import the libraries used for constructing this agent and environment\nfrom agents import *\nfrom random import *\nimport numpy as np\n\n#Create the classes of the objects placed in the enviroments\nclass Treasure1(Thing):\n    pass\n\nclass Treasure2(Thing):\n    pass\n\nclass DisposTool(Thing):\n    pass\n\nclass ReuseTool(Thing):\n    pass\n\nclass Wall(Thing):\n    pass\n\n\n\n#Create a partially observable class for the environment\nclass PartialIsland(Environment):\n    def __init__(self, width=7, height=7): #Default shape of the environment is square 7x7\n        super(PartialIsland, self).__init__()\n\n        self.width = width\n        self.height = height\n    \n    def percept(self, agent): #returns a list of objects that the given agent can sense\n        self.percepts = []\n        \n        locations = self.getPartialLocations(agent.location) #coordinates used to fetch the objects perceivable by the agent\n\n        for locus in locations:\n            things = self.list_things_at(locus)\n            for thing in things:\n                self.percepts.append(thing)\n        return self.percepts\n    \n    def getPartialLocations(self, agent_locus): #returns a list of coordinates of the cells adjacent to the current agent's position\n        here = [agent_locus[0], agent_locus[1]]\n        up = [agent_locus[0] - 1, agent_locus[1]]\n        upright = [agent_locus[0] - 1, agent_locus[1] + 1]\n        right = [agent_locus[0], agent_locus[1] + 1]\n        downright = [agent_locus[0] + 1, agent_locus[1] + 1]\n        down = [agent_locus[0] + 1, agent_locus[1]]\n        downleft = [agent_locus[0] + 1, agent_locus[1] - 1]\n        left = [agent_locus[0], agent_locus[1] - 1]\n        upleft = [agent_locus[0] - 1, agent_locus[1] - 1]\n        \n        locations = [here, up, upright, right, downright, down, downleft, left, upleft]\n        return locations\n    \n            \n    def execute_action(self, agent, action): #this function is called when running the environment after the agent's program returns an action\n        '''changes the state of the environment based on what the agent does.'''\n        \n        if action == 'moveRandom':\n            direction = randint(1, 4)\n            print(\"SELECTED ACTION: Random\")\n            if direction == 1:\n                action = 'moveRight'\n            elif direction == 2:\n                action = 'moveLeft'\n            elif direction == 3:\n                action = 'moveUp'\n            elif direction == 4:\n                action = 'moveDown'\n                '''This module handles the case when the agent percieved only walls or empty spaces in the cells adjacent to it. The agent in this\n                case decides to prepare a random movement action'''\n\n        if action == 'moveRight':\n            if agent.location[1] < 6:\n                walls = self.list_things_at([agent.location[0], agent.location[1]+1], tclass=Wall)\n                if len(walls) == 0:\n                    agent.moveRight()\n                    print(\"SELECTED ACTION: Right\")\n                else:\n                    agent.NoOp()\n            else:\n                print(\"SELECTED ACTION: canceled, tried to move out of bounds\")\n                agent.performance -= 5\n                '''This module handles the case when the agent decided to move right. First a check is performed to see if the agent decided to move out\n                of bounds, in this case the agent's location does not change in the environment and 5 points are deducted from the performance for the\n                agent's intent. If the move was in-bounds then it is checked wether or not the destination cell contains a wall; if this is the case the\n                agent's decision to move is changed to standing still, otherwise the agent moves and its location is updated in the env successfully'''\n                        \n        elif action == 'moveLeft':\n            if agent.location[1] > 1:\n                walls = self.list_things_at([agent.location[0], agent.location[1]-1], tclass=Wall)\n                if len(walls) == 0:\n                    agent.moveLeft()\n                    print(\"SELECTED ACTION: Left\")\n                else:\n                    agent.NoOp()\n            else:\n                print(\"SELECTED ACTION: canceled, tried to move out of bounds\")\n                agent.performance -= 5\n                '''This module handles the case when the agent decided to move left. It follows the same logic as the previous module'''\n        \n        elif action == 'moveUp':\n            if agent.location[0] > 1:\n                walls = self.list_things_at([agent.location[0] - 1, agent.location[1]], tclass=Wall)\n                if len(walls) == 0:\n                    agent.moveUp()\n                    print(\"SELECTED ACTION: Up\")\n                else:\n                    agent.NoOp()\n            else:\n                print(\"SELECTED ACTION: canceled, tried to move out of bounds\")\n                agent.performance -= 5\n                '''This module handles the case when the agent decided to move up. It follows the same logic as the previous module'''\n                \n        elif action == 'moveDown':\n            if agent.location[0] < 6:\n                walls = self.list_things_at([agent.location[0] + 1, agent.location[1]], tclass=Wall)\n                if len(walls) == 0:\n                    agent.moveDown()\n                    print(\"SELECTED ACTION: Down\")\n                else:\n                    agent.NoOp()\n            else:\n                print(\"SELECTED ACTION: canceled, tried to move out of bounds\")\n                agent.performance -= 5\n                '''This module handles the case when the agent decided to move down. It follows the same logic as the previous module'''\n                \n        elif action == \"Greuse\":\n            items = self.list_things_at(agent.location, tclass=ReuseTool)\n            if len(items) != 0:\n                if agent.greuse(items[0]):\n                    self.delete_thing(items[0])\n                    self.matrix[items[0].location[0]][items[0].location[1]] = '-'\n                    '''This module handles the case when the agent decides to grab a reusable tool. First a confirmation that a reusable tool object is\n                    indeed in the current agent's location is performed. Upon success in the check, the agent picks up the tool, then it is deleted from\n                    the env (internally from the array of things in the env) and finally the graphic representation of the env is modified accordingly'''\n        \n        elif action == \"Gdispos\":\n            agent.gdispos()\n            items = self.list_things_at(agent.location, tclass=DisposTool)\n            if len(items) != 0:\n                if agent.gdispos(items[0]):\n                    self.delete_thing(items[0])\n                    self.matrix[items[0].location[0]][items[0].location[1]] = '-'\n                    '''This module handles the case when the agent decides to grab a disposable tool. It follows the same logic as the previous module'''\n        \n        elif action == \"GTreasure1\":\n            items = self.list_things_at(agent.location, tclass=Treasure1)\n            if len(items) != 0:\n                if agent.gTreasure1(items[0]): \n                    self.delete_thing(items[0])\n                    self.matrix[items[0].location[0]][items[0].location[1]] = '-'\n                    '''This module handles the case when the agent decides to grab a type 1 treasure. It follows the same logic as the previous module'''\n        \n        elif action == \"GTreasure2\":\n            items = self.list_things_at(agent.location, tclass=Treasure2)\n            if len(items) != 0:\n                if agent.gTreasure2(items[0]):\n                    self.delete_thing(items[0])\n                    self.matrix[items[0].location[0]][items[0].location[1]] = '-'\n                    '''This module handles the case when the agent decides to grab a type 2 treasure. It follows the same logic as the previous module'''\n        \n        elif action == \"NoOp\":\n            pass\n        \n        #Report the modified environment along with agent status\n        print(\"NEW AGENT'S PERFORMANCE: \" + str(agent.performance))\n        print(\"NEW ENVIRONMENT STATE\")\n        print(\"Agent location: \" + str(agent.location))\n        print(\"Agent tools: \" + str(agent.holding))\n        print('\\n'.join([''.join(['{:3}'.format(item) for item in row]) \n              for row in self.matrix]))\n    \n    def run(self, steps=50): #AIMA function overriden to create a graphical representation of the environment and report starting status\n        \"Run the Environment for given number of time steps.\"\n        print(\"SIMPLE REFLEX AGENT in PARTIALLY OBSERVABLE ENVIRONMENT\")\n        print(\"<STARTING>\")\n        print(\"Agent location: \" + str(self.things[0].location))\n        print(\"Agent tools: \" + str(self.things[0].holding))\n        \n        self.matrix = templateEnv(size = 6) #create an empty matrix for representing the environment graphically\n        self.matrix = fillEnv(self.matrix, self.things) #fill the matrix with objects in the environment\n        print(\"Agent performance: \" + str(self.things[0].performance))\n        \n        for step in range(steps):\n            if self.is_done():\n                print(\"\\nFINAL AGENT's PERFORMANCE: \" + str(self.things[0].performance)) #print the performance after last step of the run\n                return\n            print(\"\\n<STEP\" + str(step + 1) + \">\") #print step number starting from 1\n            self.step()\n        \n        print(\"\\nFINAL AGENT's PERFORMANCE: \" + str(self.things[0].performance)) #print the performance after last step of the run\n        \n    def step(self): #AIMA function overriden to access agent status in each step of a run and print the performance\n        \"\"\"Run the environment for one time step. If the\n        actions and exogenous changes are independent, this method will\n        do.  If there are interactions between them, you'll need to\n        override this method.\"\"\"\n        if not self.is_done():\n            actions = []\n            for agent in self.agents:\n                if agent.alive:\n                    actions.append(agent.program(agent, self.percept(agent)))\n                else:\n                    actions.append(\"\")\n                \n            #Print a graphical matrix representation of what the agent was percieving along with its status at that moment\n            print(\"PERCEPT\")\n            print(\"Agent location: \" + str(agent.location))\n            print(\"Agent tools: \" + str(agent.holding))\n            percept_repr = templateEnv(size = 6)\n            percept_repr = getUnknowns(percept_repr, agent.location)\n            fillEnv(percept_repr, self.percepts)\n                \n            for (agent, action) in zip(self.agents, actions):\n                self.execute_action(agent, action)\n            self.exogenous_change()\n            \n    def is_done(self): #AIMA function overriden to terminate the run if there are no objects of interest left in the environment\n        no_edibles = not any(isinstance(thing, Treasure1) or isinstance(thing, DisposTool) or isinstance(thing, ReuseTool) or isinstance(thing, Treasure2) for thing in self.things)\n        dead_agents = not any(agent.is_alive() for agent in self.agents)\n        return dead_agents or no_edibles\n    \n    \n#Create a simple reflex agent for use in an island environment\nclass ReflexHunter(Agent):\n    def __init__(self, program=None): #default state of the agent\n        self.alive = True\n        self.bump = False\n        self.holding = []\n        self.performance = 50\n        if program is None:\n            def program(percept):\n                return eval(input('Percept={}; action? ' .format(percept)))\n        assert isinstance(program, collections.Callable)\n        self.program = program\n        \n    #The following 4 methods are actions that the agent executes when it wants to move. Which move of the four is called depends on the calculations\n    #made in the reflexProgram() function and on wether or not the action is valid in the current state of the environment as seen in execute_action()\n    #in the environment's methods. Performance is reduced by one point in all cases\n    def moveRight(self):\n        self.performance -= 1\n        self.location[1] += 1\n\n    def moveLeft(self):\n        self.performance -= 1\n        self.location[1] -= 1\n\n    def moveUp(self):\n        self.performance -= 1\n        self.location[0] -= 1\n\n    def moveDown(self):\n        self.performance -= 1\n        self.location[0] += 1\n\n    #The following 4 functions are actions that the agent executes when it wants to grab something. They operate in the a similar fashion as the\n    #movement methods except that the agent's inventory is also manipulated\n    def greuse(self, thing):\n        self.performance -= 1\n        if isinstance(thing, ReuseTool):\n            print(\"SELECTED ACTION: Greuse\")\n            self.holding.append('H')\n            return True\n        return False\n\n    def gdispos(self, thing = None):\n        self.performance -= 1\n        if isinstance(thing, DisposTool):\n            print(\"SELECTED ACTION: Gdispos\")\n            self.holding.append('h')\n            return True\n        return False\n\n    def gTreasure1(self, thing):\n        if isinstance(thing, Treasure1):\n            print(\"SELECTED ACTION: GTreas1\")\n            self.performance += 20\n            if 'H' in self.holding == False:\n                self.holding.remove('h')\n            return True\n        return False\n\n    def gTreasure2(self, thing):\n        if isinstance(thing, Treasure2):\n            print(\"SELECTED ACTION: GTreas2\")\n            self.performance += 40\n            self.holding.remove('h')\n            return True\n        return False\n    \n    #This action is performed when there is a wall at a cell where the agent wants to move to. Mantains the agent's location for the current step\n    def NoOp(self):\n        print(\"SELECTED ACTION: NoOp saw a wall\")\n        return False\n    \n    \ndef reflexProgram(agent, percepts):\n    '''returns a string dictating the action to be performed by the agent and ultimately what needs to be modified in environment'''\n    actionTaken = False\n    \n    for p in percepts:\n        if actionTaken:\n            break\n            \n        in_location = agent.location == p.location #used to check the agent's current location for a grabable object\n        \n        if isinstance(p, Treasure1) and ('h' in agent.holding or 'H' in agent.holding):\n            if in_location:\n                actionTaken = True\n                return 'GTreasure1'\n            else:\n                moveTo = getDirection(agent.location, p.location)\n                actionTaken = True\n                return moveTo\n                '''This module handles when the agent is able to sense a type 1 treasure. First check if the agent has a any kind of tool to grab the\n                treasure: if not then let this iteration of the loop pass because there is no way to obtain the treasure right now (a new percept is\n                checked in the next iteration), if yes check if the treasure is at the same place as the agent, if not move towards it by calling the\n                getDirection() function, if yes output the grab action'''\n        \n        elif isinstance(p, Treasure2) and 'h' in agent.holding:\n            if in_location:\n                actionTaken = True\n                return 'GTreasure2'\n            else:\n                moveTo = getDirection(agent.location, p.location)\n                actionTaken = True\n                return moveTo\n                '''This module handles when the agent is able to sense a type 2 treasure. It works in the same fashion as the last module with the\n                difference being that only disposable tools on inventory are considered in the first condition check and not any kind of tool as in\n                the previous module'''\n            \n        elif isinstance(p, DisposTool):\n            if in_location:\n                actionTaken = True\n                return 'Gdispos'\n            else:\n                moveTo = getDirection(agent.location, p.location)\n                actionTaken = True\n                return moveTo\n                '''This module handles when the agent is able to sense a disposable tool. I simply checks if the tool is in the current location of the\n                agent to pick it up or otherwise move towards it calling the getDirection() function'''\n            \n        elif isinstance(p, ReuseTool):\n            if in_location:\n                actionTaken = True\n                return 'Greuse'\n            else:\n                moveTo = getDirection(agent.location, p.location)\n                actionTaken = True\n                return moveTo\n                '''This module handles when the agent is able to sense a reusable tool. Works the same as the last module'''\n    \n    if not actionTaken:\n        return 'moveRandom'\n    \ndef getDirection(origin, goal):\n    '''Decides an action that will move the agent one cell closer (in manhatan distance) to an object it wants to pick up'''\n    if origin[1] < goal[1]:\n        return 'moveRight'\n    elif origin[1] > goal[1]:\n        return 'moveLeft'\n    elif origin [0] > goal[0]:\n        return 'moveUp'\n    else:\n        return 'moveDown'\n    \ndef templateEnv(size):\n    '''Creates an empty matrix to represent an enviroment or agent's percept'''\n    matrix = np.array([['-' for i in range (0, size + 2)] for j in range (0, size + 2)])\n    \n    #Make the matrix pretty with outer rol and col aesthetics\n    for i in range(0, size + 2):\n        for j in range(0, size + 2):\n            if (i == 0 and j == 0) or (i == size + 1 and j == size + 1):\n                matrix[i][j] = '\\\\'\n\n            elif (i == 0 and j == size + 1) or (i == size + 1 and j == 0):\n                matrix[i][j] = '/'\n\n            elif i == 0 and (j != 0 or j!= size + 1):\n                matrix[i][j] = str(j)\n            \n            elif i == size + 1 and (j != 0 or j!= size + 1):\n                matrix[i][j] = str(j)\n\n            elif (i != 0 or i != size + 1) and j == 0:\n                matrix[i][j] = str(i)\n            \n            elif (i != 0 or i != size + 1) and j == size + 1:\n                matrix[i][j] = str(i)\n    return matrix\n\ndef fillEnv(matrix, things):\n    '''fills an empty matrix with objects in the environment at their proper locations'''\n    \n    for thing in things:\n        if isinstance(thing, Treasure1):\n            matrix[thing.location[0]][thing.location[1]] = 'T'\n        elif isinstance(thing, Treasure2):\n            matrix[thing.location[0]][thing.location[1]] = 't'\n        elif isinstance(thing, DisposTool):\n            matrix[thing.location[0]][thing.location[1]] = 'h'\n        elif isinstance(thing, ReuseTool):\n            matrix[thing.location[0]][thing.location[1]] = 'H'\n        elif isinstance(thing, Wall):\n            matrix[thing.location[0]][thing.location[1]] = 'X'\n        \n    print('\\n'.join([''.join(['{:3}'.format(item) for item in row]) \n           for row in matrix]))\n    return matrix\n    \ndef getUnknowns(matrix, reference):\n    '''modifies the graphical representation of the agent's percept to reflect the cells it can't sense (for partial env)'''\n    for i in range(1, len(matrix) - 1):\n            for j in range(1, len(matrix) - 1):\n                \n                if [i, j] != reference:\n                    if [i+1, j] != reference and [i-1, j] != reference:\n                        if [i, j+1] != reference and [i, j-1] != reference:\n                            if [i+1, j+1] != reference and [i-1, j-1] != reference:\n                                if [i+1, j-1] != reference and [i-1, j+1] != reference:\n                                    matrix[i][j] = '?'\n    return matrix\n\n\ndef main():\n    '''Run the simple reflex hunter in the partial treasure island environment'''\n    partial_island = PartialIsland()\n    mike = ReflexHunter(reflexProgram)\n\n    #Create 2 objects of each kind to place on the environment\n    treasure1A = Treasure1()\n    treasure1B = Treasure1()\n    treasure1C = Treasure1()\n    treasure2A = Treasure2()\n    treasure2B = Treasure2()\n    treasure2C = Treasure2()\n    dispos1 = DisposTool()\n    dispos2 = DisposTool()\n    dispos3 = DisposTool()\n    reusable1 = ReuseTool()\n    reusable2 = ReuseTool()\n    reusable3 = ReuseTool()\n    wall = Wall()\n    wall2 = Wall()\n    wall3 = Wall()\n\n    partial_island.add_thing(mike, [1,1])\n    mike.performance = 50 #prevents AIMA from resetting the performance to 0 when adding the agent to the environment\n    \n    #add things preventing placing more than one item in a single cell in the environment\n    xy = [[1, 1],[1, 2],[1, 3],[1, 4],[1, 5],[1, 6],\n          [2, 1],[2, 2],[2, 3],[2, 4],[2, 5],[2, 6],\n          [3, 1],[3, 2],[3, 3],[3, 4],[3, 5],[3, 6],\n          [4, 1],[4, 2],[4, 3],[4, 4],[4, 5],[4, 6],\n          [5, 1],[5, 2],[5, 3],[5, 4],[5, 5],[5, 6],\n          [6, 1],[6, 2],[6, 3],[6, 4],[6, 5],[6, 6]]\n\n    partial_island.add_thing(treasure1A, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(treasure1B, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(treasure1C, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(reusable1, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(reusable2, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(reusable3, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(treasure2A, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(treasure2B, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(treasure2C, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(dispos1, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(dispos2, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(dispos3, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(wall, xy.pop(randint(0, len(xy)) - 1))\n    partial_island.add_thing(wall2, xy.pop(randint(0, len(xy) - 1)))\n    partial_island.add_thing(wall3, xy.pop(randint(0, len(xy) - 1)))\n\n    partial_island.run(50) #run the program for 50 iterations\n    return mike.performance #for studying several runs\n                                          \nmain()",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "SIMPLE REFLEX AGENT in PARTIALLY OBSERVABLE ENVIRONMENT\n<STARTING>\nAgent location: [1, 1]\nAgent tools: []\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  h  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \nAgent performance: 50\n\n<STEP1>\nPERCEPT\nAgent location: [1, 1]\nAgent tools: []\n\\  1  2  3  4  5  6  /  \n1  t  -  ?  ?  ?  ?  1  \n2  H  h  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 49\nNEW ENVIRONMENT STATE\nAgent location: [1, 2]\nAgent tools: []\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  h  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP2>\nPERCEPT\nAgent location: [1, 2]\nAgent tools: []\n\\  1  2  3  4  5  6  /  \n1  t  -  -  ?  ?  ?  1  \n2  H  h  -  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 48\nNEW ENVIRONMENT STATE\nAgent location: [2, 2]\nAgent tools: []\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  h  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP3>\nPERCEPT\nAgent location: [2, 2]\nAgent tools: []\n\\  1  2  3  4  5  6  /  \n1  t  -  -  ?  ?  ?  1  \n2  H  h  -  ?  ?  ?  2  \n3  -  H  T  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Gdispos\nNEW AGENT'S PERFORMANCE: 46\nNEW ENVIRONMENT STATE\nAgent location: [2, 2]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP4>\nPERCEPT\nAgent location: [2, 2]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  ?  ?  ?  1  \n2  H  -  -  ?  ?  ?  2  \n3  -  H  T  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 45\nNEW ENVIRONMENT STATE\nAgent location: [2, 3]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP5>\nPERCEPT\nAgent location: [2, 3]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  -  -  T  ?  ?  1  \n2  ?  -  -  -  ?  ?  2  \n3  ?  H  T  T  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 44\nNEW ENVIRONMENT STATE\nAgent location: [2, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP6>\nPERCEPT\nAgent location: [2, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  -  T  -  ?  1  \n2  ?  ?  -  -  X  ?  2  \n3  ?  ?  T  T  -  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 43\nNEW ENVIRONMENT STATE\nAgent location: [1, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  T  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP7>\nPERCEPT\nAgent location: [1, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  -  T  -  ?  1  \n2  ?  ?  -  -  X  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: GTreas1\nNEW AGENT'S PERFORMANCE: 63\nNEW ENVIRONMENT STATE\nAgent location: [1, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP8>\nPERCEPT\nAgent location: [1, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  -  -  -  ?  1  \n2  ?  ?  -  -  X  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 62\nNEW ENVIRONMENT STATE\nAgent location: [2, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP9>\nPERCEPT\nAgent location: [2, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  -  -  -  ?  1  \n2  ?  ?  -  -  X  ?  2  \n3  ?  ?  T  T  -  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 61\nNEW ENVIRONMENT STATE\nAgent location: [3, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  T  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP10>\nPERCEPT\nAgent location: [3, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  -  -  X  ?  2  \n3  ?  ?  T  T  -  ?  3  \n4  ?  ?  -  X  -  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: GTreas1\nNEW AGENT'S PERFORMANCE: 81\nNEW ENVIRONMENT STATE\nAgent location: [3, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP11>\nPERCEPT\nAgent location: [3, 4]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  -  -  X  ?  2  \n3  ?  ?  T  -  -  ?  3  \n4  ?  ?  -  X  -  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 80\nNEW ENVIRONMENT STATE\nAgent location: [3, 3]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  T  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP12>\nPERCEPT\nAgent location: [3, 3]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  -  -  -  ?  ?  2  \n3  ?  H  T  -  ?  ?  3  \n4  ?  -  -  X  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: GTreas1\nNEW AGENT'S PERFORMANCE: 100\nNEW ENVIRONMENT STATE\nAgent location: [3, 3]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP13>\nPERCEPT\nAgent location: [3, 3]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  -  -  -  ?  ?  2  \n3  ?  H  -  -  ?  ?  3  \n4  ?  -  -  X  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 99\nNEW ENVIRONMENT STATE\nAgent location: [3, 2]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  H  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP14>\nPERCEPT\nAgent location: [3, 2]\nAgent tools: ['h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  H  -  -  ?  ?  ?  2  \n3  -  H  -  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Greuse\nNEW AGENT'S PERFORMANCE: 98\nNEW ENVIRONMENT STATE\nAgent location: [3, 2]\nAgent tools: ['h', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP15>\nPERCEPT\nAgent location: [3, 2]\nAgent tools: ['h', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  H  -  -  ?  ?  ?  2  \n3  -  -  -  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 97\nNEW ENVIRONMENT STATE\nAgent location: [3, 1]\nAgent tools: ['h', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP16>\nPERCEPT\nAgent location: [3, 1]\nAgent tools: ['h', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  H  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 96\nNEW ENVIRONMENT STATE\nAgent location: [2, 1]\nAgent tools: ['h', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  H  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP17>\nPERCEPT\nAgent location: [2, 1]\nAgent tools: ['h', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  ?  ?  ?  ?  1  \n2  H  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Greuse\nNEW AGENT'S PERFORMANCE: 95\nNEW ENVIRONMENT STATE\nAgent location: [2, 1]\nAgent tools: ['h', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP18>\nPERCEPT\nAgent location: [2, 1]\nAgent tools: ['h', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 94\nNEW ENVIRONMENT STATE\nAgent location: [1, 1]\nAgent tools: ['h', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP19>\nPERCEPT\nAgent location: [1, 1]\nAgent tools: ['h', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  t  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: GTreas2\nNEW AGENT'S PERFORMANCE: 134\nNEW ENVIRONMENT STATE\nAgent location: [1, 1]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP20>\nPERCEPT\nAgent location: [1, 1]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 133\nNEW ENVIRONMENT STATE\nAgent location: [1, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP21>\nPERCEPT\nAgent location: [1, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  ?  ?  ?  1  \n2  -  -  -  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 132\nNEW ENVIRONMENT STATE\nAgent location: [2, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP22>\nPERCEPT\nAgent location: [2, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  ?  ?  ?  1  \n2  -  -  -  ?  ?  ?  2  \n3  -  -  -  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 131\nNEW ENVIRONMENT STATE\nAgent location: [3, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP23>\nPERCEPT\nAgent location: [3, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  -  -  -  ?  ?  ?  2  \n3  -  -  -  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 130\nNEW ENVIRONMENT STATE\nAgent location: [3, 3]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP24>\nPERCEPT\nAgent location: [3, 3]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  -  -  -  ?  ?  2  \n3  ?  -  -  -  ?  ?  3  \n4  ?  -  -  X  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 129\nNEW ENVIRONMENT STATE\nAgent location: [4, 3]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP25>\nPERCEPT\nAgent location: [4, 3]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  -  -  -  ?  ?  3  \n4  ?  -  -  X  ?  ?  4  \n5  ?  h  -  -  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 128\nNEW ENVIRONMENT STATE\nAgent location: [4, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP26>\nPERCEPT\nAgent location: [4, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  -  -  -  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  t  h  -  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 127\nNEW ENVIRONMENT STATE\nAgent location: [5, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  h  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP27>\nPERCEPT\nAgent location: [5, 2]\nAgent tools: ['H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  t  h  -  ?  ?  ?  5  \n6  -  -  H  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Gdispos\nNEW AGENT'S PERFORMANCE: 125\nNEW ENVIRONMENT STATE\nAgent location: [5, 2]\nAgent tools: ['H', 'H', 'h']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP28>\nPERCEPT\nAgent location: [5, 2]\nAgent tools: ['H', 'H', 'h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  t  -  -  ?  ?  ?  5  \n6  -  -  H  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 124\nNEW ENVIRONMENT STATE\nAgent location: [5, 3]\nAgent tools: ['H', 'H', 'h']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP29>\nPERCEPT\nAgent location: [5, 3]\nAgent tools: ['H', 'H', 'h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  -  -  X  ?  ?  4  \n5  ?  -  -  -  ?  ?  5  \n6  ?  -  H  -  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 123\nNEW ENVIRONMENT STATE\nAgent location: [6, 3]\nAgent tools: ['H', 'H', 'h']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  H  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP30>\nPERCEPT\nAgent location: [6, 3]\nAgent tools: ['H', 'H', 'h']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  -  -  -  ?  ?  5  \n6  ?  -  H  -  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Greuse\nNEW AGENT'S PERFORMANCE: 122\nNEW ENVIRONMENT STATE\nAgent location: [6, 3]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP31>\nPERCEPT\nAgent location: [6, 3]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  -  -  -  ?  ?  5  \n6  ?  -  -  -  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 121\nNEW ENVIRONMENT STATE\nAgent location: [6, 2]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP32>\nPERCEPT\nAgent location: [6, 2]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  t  -  -  ?  ?  ?  5  \n6  -  -  -  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 120\nNEW ENVIRONMENT STATE\nAgent location: [6, 1]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP33>\nPERCEPT\nAgent location: [6, 1]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  t  -  ?  ?  ?  ?  5  \n6  -  -  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 119\nNEW ENVIRONMENT STATE\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  t  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP34>\nPERCEPT\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'h', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  t  -  ?  ?  ?  ?  5  \n6  -  -  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: GTreas2\nNEW AGENT'S PERFORMANCE: 159\nNEW ENVIRONMENT STATE\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP35>\nPERCEPT\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  -  -  ?  ?  ?  ?  5  \n6  -  -  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: canceled, tried to move out of bounds\nNEW AGENT'S PERFORMANCE: 154\nNEW ENVIRONMENT STATE\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP36>\nPERCEPT\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  -  -  ?  ?  ?  ?  5  \n6  -  -  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: canceled, tried to move out of bounds\nNEW AGENT'S PERFORMANCE: 149\nNEW ENVIRONMENT STATE\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP37>\nPERCEPT\nAgent location: [5, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  -  -  ?  ?  ?  ?  5  \n6  -  -  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 148\nNEW ENVIRONMENT STATE\nAgent location: [4, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP38>\nPERCEPT\nAgent location: [4, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  ?  ?  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  -  -  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 147\nNEW ENVIRONMENT STATE\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP39>\nPERCEPT\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: canceled, tried to move out of bounds\nNEW AGENT'S PERFORMANCE: 142\nNEW ENVIRONMENT STATE\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP40>\nPERCEPT\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 141\nNEW ENVIRONMENT STATE\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP41>\nPERCEPT\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 140\nNEW ENVIRONMENT STATE\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP42>\nPERCEPT\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Right\nNEW AGENT'S PERFORMANCE: 139\nNEW ENVIRONMENT STATE\nAgent location: [3, 2]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP43>\nPERCEPT\nAgent location: [3, 2]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  -  -  -  ?  ?  ?  2  \n3  -  -  -  ?  ?  ?  3  \n4  -  -  -  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Left\nNEW AGENT'S PERFORMANCE: 138\nNEW ENVIRONMENT STATE\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP44>\nPERCEPT\nAgent location: [3, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  ?  ?  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  -  -  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 137\nNEW ENVIRONMENT STATE\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP45>\nPERCEPT\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: canceled, tried to move out of bounds\nNEW AGENT'S PERFORMANCE: 132\nNEW ENVIRONMENT STATE\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP46>\nPERCEPT\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 131\nNEW ENVIRONMENT STATE\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP47>\nPERCEPT\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: canceled, tried to move out of bounds\nNEW AGENT'S PERFORMANCE: 126\nNEW ENVIRONMENT STATE\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP48>\nPERCEPT\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: canceled, tried to move out of bounds\nNEW AGENT'S PERFORMANCE: 121\nNEW ENVIRONMENT STATE\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP49>\nPERCEPT\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  ?  ?  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Down\nNEW AGENT'S PERFORMANCE: 120\nNEW ENVIRONMENT STATE\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\n<STEP50>\nPERCEPT\nAgent location: [2, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  ?  ?  ?  ?  1  \n2  -  -  ?  ?  ?  ?  2  \n3  -  -  ?  ?  ?  ?  3  \n4  ?  ?  ?  ?  ?  ?  4  \n5  ?  ?  ?  ?  ?  ?  5  \n6  ?  ?  ?  ?  ?  ?  6  \n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/  1  2  3  4  5  6  \\  \nSELECTED ACTION: Random\nSELECTED ACTION: Up\nNEW AGENT'S PERFORMANCE: 119\nNEW ENVIRONMENT STATE\nAgent location: [1, 1]\nAgent tools: ['H', 'H', 'H']\n\\  1  2  3  4  5  6  /  \n1  -  -  -  -  -  t  1  \n2  -  -  -  -  X  h  2  \n3  -  -  -  -  -  -  3  \n4  -  -  -  X  -  -  4  \n5  -  -  -  -  -  -  5  \n6  -  -  -  -  X  -  6  \n/  1  2  3  4  5  6  \\  \n\nFINAL AGENT's PERFORMANCE: 119\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/plain": "119"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next cell executes 100 runs of 50 iterations each of the previous agent and environment. The average agent performance of the runs is computed"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%capture\ntotal_performance = 0\nfor i in range (1, 100):\n    run_result = main();\n    total_performance += run_result\naverage_performance = total_performance / 100",
      "execution_count": 68,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next cell prints the average agent performance from the last experiment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "average_performance",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 69,
          "data": {
            "text/plain": "77.43"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}